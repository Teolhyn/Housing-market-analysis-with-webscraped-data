{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae1e6f30",
   "metadata": {},
   "source": [
    "# Hobby project: Webscraping etuovi.com to get data for analyzing the housing market in Finland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4dc50b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # this module helps in web scrapping.\n",
    "import requests  # this module helps us to download a web page.\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5f3b3b",
   "metadata": {},
   "source": [
    "## Scraping the hrefs of all of the house adverts (cards) that were listed in Finland.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c124188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:46<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "#TODO: save hrefs to file so they can be opened when developing the next cell.\n",
    "\n",
    "# Download webpage (etuovi.com, Turku).\n",
    "url = \"https://www.etuovi.com/myytavat-asunnot?haku=M1857661494\"\n",
    "# Download contents in text format.\n",
    "data = requests.get(url).text\n",
    "\n",
    "soup = BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "# Firstly, lets get the amount of pages we need to go through:\n",
    "no_pages = 50\n",
    "# no_pages = int([i.string for i in soup.find('div', class_= 'Pagination__Col-sc-3ydysw-1 kpHoDY').next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_element][0])\n",
    "houses_href = []\n",
    "\n",
    "for i in tqdm(range(1,no_pages + 1)):\n",
    "    # Download webpage (etuovi.com, Turku).\n",
    "    url = \"https://www.etuovi.com/myytavat-asunnot?haku=M1857661494&sivu=\" + str(i)\n",
    "    # Download contents in text format.\n",
    "    data = requests.get(url).text\n",
    "\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "    # Scrape all links on page\n",
    "\n",
    "    project_href = [i['href'] for i in soup.find_all('a', href=True)]\n",
    "    # print(project_href)\n",
    "\n",
    "    #Filter out links that are not houses for sale\n",
    "\n",
    "    for i in project_href:\n",
    "        if i.startswith('/kohde'):\n",
    "            houses_href.append(i)\n",
    "\n",
    "    time.sleep(0.25) #Lets not overload servers\n",
    "    # print(houses_href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56590ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of links before: 1497\n",
      "Amount of links after removing duplicates: 1497\n"
     ]
    }
   ],
   "source": [
    "# Just to make sure lets remove possible duplicates.\n",
    "print(\"Amount of links before:\", len(houses_href))\n",
    "\n",
    "houses_href = list(dict.fromkeys(houses_href))\n",
    "print(\"Amount of links after removing duplicates:\", len(houses_href))\n",
    "\n",
    "# print(houses_href)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5daf30a",
   "metadata": {},
   "source": [
    "## Scraping data from the adverts\n",
    "This part iterates through all of the adverts and looks for values like type, address, price, size and year. In the end it transforms it to a pandas dataframe. NOTE: This iteration is quite large and it takes approximately 10 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27cd7c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1022/47833 [12:59<10:13:55,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved a backup of  1000  iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2035/47833 [26:03<10:47:05,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved a backup of  2000  iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 3061/47833 [39:09<9:53:04,  1.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved a backup of  3000  iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 4083/47833 [52:30<9:34:42,  1.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved a backup of  4000  iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5097/47833 [1:05:37<9:35:33,  1.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved a backup of  5000  iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 6116/47833 [1:18:50<9:35:32,  1.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved a backup of  6000  iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 7147/47833 [1:33:04<8:56:47,  1.26it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved a backup of  7000  iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 8205/47833 [1:48:25<7:40:30,  1.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved a backup of  8000  iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 9247/47833 [2:01:39<9:13:46,  1.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved a backup of  9000  iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 10262/47833 [2:19:46<7:53:37,  1.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved a backup of  10000  iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 11299/47833 [2:32:49<7:58:34,  1.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved a backup of  11000  iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 12059/47833 [2:42:43<8:29:39,  1.17it/s] "
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: Optimize memory usage, perhaps use batches or something? Check that types, it could also be reason for very high memory usage. (After doing data refinement, I noticed that the types are all objects which propably causes high memory usage)\n",
    "# TODO: Handle ConnectionErrors.\n",
    "\n",
    "\n",
    "houses_for_sale = []\n",
    "iterations = 0\n",
    "# for href in itertools.islice(houses_href, 0, 30): # For testing\n",
    "for href in tqdm(houses_href):\n",
    "\n",
    "    url = \"https://www.etuovi.com/\" + str(href)\n",
    "\n",
    "    data = requests.get(url).text\n",
    "\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "    # Type, rooms, kitchen, bathroom etc.\n",
    "    try:\n",
    "        house_type_etc = [i.string for i in soup.find('div', class_='flexboxgrid__col-xs-12__1I1LS flexboxgrid__col-md-6__1n8OT ItemSummaryContainer__alignLeft__2IE5Z')][0]\n",
    "    except:\n",
    "        TypeError\n",
    "        house_type_etc = None\n",
    "        continue\n",
    "\n",
    "\n",
    "    # City\n",
    "    try:\n",
    "        house_city = [i.string for i in soup.find('a', class_='MuiTypography-root MuiLink-root MuiLink-underlineHover InfoSegment__noStyleLink__2e28Y MuiTypography-colorPrimary')][0]\n",
    "    except:\n",
    "        TypeError\n",
    "        house_city = None\n",
    "        continue\n",
    "\n",
    "\n",
    "    #Area  \n",
    "    try:\n",
    "        house_area = [i.string for i in soup.find('a', class_='MuiTypography-root MuiLink-root MuiLink-underlineHover InfoSegment__noStyleLink__2e28Y MuiTypography-colorPrimary').nextSibling][1]\n",
    "    except:\n",
    "        TypeError\n",
    "        house_area = None\n",
    "        continue \n",
    "\n",
    "\n",
    "    #Address\n",
    "    try:\n",
    "        house_address = [i.string for i in soup.find('a', class_='MuiTypography-root MuiLink-root MuiLink-underlineHover InfoSegment__noStyleLink__2e28Y MuiTypography-colorPrimary').nextSibling.parent.nextSibling][0]\n",
    "    except:\n",
    "        TypeError\n",
    "        house_address = None\n",
    "        continue \n",
    "\n",
    "\n",
    "    # Price\n",
    "    try:\n",
    "        house_price = [i.string for i in soup.find('div', class_='flexboxgrid__col-xs-4__p2Lev flexboxgrid__col-md-5__3SFMx')][1]\n",
    "    except:\n",
    "        TypeError\n",
    "        house_price = None\n",
    "        continue\n",
    "\n",
    "    # Take only the number, not unit.\n",
    "    if house_price is None:\n",
    "        house_price = None \n",
    "    else: house_price_number = house_price[0:(len(house_price)-2)]\n",
    "\n",
    "\n",
    "    # Size\n",
    "    try:\n",
    "        house_size = [i.string for i in soup.find('div', class_='flexboxgrid__col-xs-4__p2Lev flexboxgrid__col-md-4__2DYW-')][1]\n",
    "    except:\n",
    "        TypeError\n",
    "        house_size = None\n",
    "        continue\n",
    "\n",
    "    # Take only the number, not unit.\n",
    "    if house_size is None:\n",
    "        house_size_number = None\n",
    "    else: house_size_number = house_size.rsplit(' ', 100)[0]\n",
    "\n",
    "\n",
    "    # Year\n",
    "    try:\n",
    "        house_year = [i.string for i in soup.find('div', class_='flexboxgrid__col-xs-4__p2Lev flexboxgrid__col-md-3__1YPhN')][1]\n",
    "    except:\n",
    "        TypeError\n",
    "        house_year = None\n",
    "        continue\n",
    "\n",
    "    # Form of ownership\n",
    "    try:\n",
    "        house_form_of_ownership = [i.string for i in soup.find('div', class_='CompactInfoRow__infoRow__2hjs_ flexboxgrid__row__wfmuy').next_sibling.next_sibling.next_sibling][1]\n",
    "    except:\n",
    "        TypeError\n",
    "        house_form_of_ownership = None\n",
    "        continue\n",
    "\n",
    "\n",
    "    # \"house_type_etc\" is now in format type | number of rooms + etc. + etc.\n",
    "    # Lets divide it to own categories\n",
    "    house_type_etc = str(house_type_etc)\n",
    "    # house_type = house_type_etc.rsplit(' ', 100)[0]\n",
    "\n",
    "    houses_for_sale.append({\"Type\":house_type_etc,\"Address\":house_address, \"District\":house_area ,\"City\":house_city, \"Price (€)\":house_price_number, \"Area (sqm.)\":house_size_number, \"Year built\" : house_year, \"Form of ownership\" : house_form_of_ownership})\n",
    "    iterations += 1\n",
    "    if iterations % 1000 == 0:\n",
    "        houses_df = pd.DataFrame(houses_for_sale)\n",
    "        filename = 'BACKUP-' + str(iterations/1000) + 'K-iterations-' + date.today().strftime(\"%b-%d-%Y\") + '-house-market-data.csv'\n",
    "        houses_df.to_csv(filename, encoding=\"utf-8-sig\")\n",
    "        print(\"Saved a backup of \", iterations, \" iterations.\")\n",
    "    time.sleep(0.25)\n",
    "\n",
    "houses_df = pd.DataFrame(houses_for_sale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e5887",
   "metadata": {},
   "source": [
    "## Saving the scraped data to a .csv file for later refinement and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe670e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jun-30-2022-house-market-data.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'houses_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Teemu\\Desktop\\Housing_market_analysis\\Housing-market-analysis-with-webscraped-data\\Webscraper.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Teemu/Desktop/Housing_market_analysis/Housing-market-analysis-with-webscraped-data/Webscraper.ipynb#ch0000008?line=2'>3</a>\u001b[0m filename \u001b[39m=\u001b[39m bdy \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-house-market-data.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Teemu/Desktop/Housing_market_analysis/Housing-market-analysis-with-webscraped-data/Webscraper.ipynb#ch0000008?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(filename)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Teemu/Desktop/Housing_market_analysis/Housing-market-analysis-with-webscraped-data/Webscraper.ipynb#ch0000008?line=4'>5</a>\u001b[0m houses_df\u001b[39m.\u001b[39mto_csv(filename, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8-sig\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Teemu/Desktop/Housing_market_analysis/Housing-market-analysis-with-webscraped-data/Webscraper.ipynb#ch0000008?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(houses_df)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'houses_df' is not defined"
     ]
    }
   ],
   "source": [
    "today = date.today()\n",
    "bdy = today.strftime(\"%b-%d-%Y\")\n",
    "filename = bdy + '-house-market-data.csv'\n",
    "print(filename)\n",
    "houses_df.to_csv(filename, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(houses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27b17e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Webscraping and Data Science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "735f910f74695c6de02b2f60c5488624593c6ef6110038a08f3abc746094daf6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

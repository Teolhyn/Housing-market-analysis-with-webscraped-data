{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae1e6f30",
   "metadata": {},
   "source": [
    "# Hobby project: Webscraping etuovi.com to get data for analyzing the housing market in Finland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4dc50b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # this module helps in web scrapping.\n",
    "import requests  # this module helps us to download a web page.\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5f3b3b",
   "metadata": {},
   "source": [
    "## Scraping the hrefs of all of the house adverts (cards) that were listed in Finland.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c124188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1593/1593 [19:15<00:00,  1.38it/s]\n"
     ]
    }
   ],
   "source": [
    "/**\n",
    " * @todo Make the program save backups at certain intervals e.g. every 1000 iterations\n",
    " * @body Incase there are unexpected errors, the program should save backups so that hours of scraping do not go to waste\n",
    " */\n",
    "\n",
    "/**\n",
    " * @todo Handle ConnectionErrors.\n",
    " * @body -\n",
    " */\n",
    "\n",
    "# Download webpage (etuovi.com, Turku).\n",
    "url = \"https://www.etuovi.com/myytavat-asunnot?haku=M1857661494\"\n",
    "# Download contents in text format.\n",
    "data = requests.get(url).text\n",
    "\n",
    "soup = BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "# Firstly, lets get the amount of pages we need to go through:\n",
    "no_pages = int([i.string for i in soup.find('div', class_= 'Pagination__Col-sc-3ydysw-1 kpHoDY').next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_element][0])\n",
    "\n",
    "houses_href = []\n",
    "\n",
    "for i in tqdm(range(1,no_pages + 1)):\n",
    "    # Download webpage (etuovi.com, Turku).\n",
    "    url = \"https://www.etuovi.com/myytavat-asunnot?haku=M1857661494&sivu=\" + str(i)\n",
    "    # Download contents in text format.\n",
    "    data = requests.get(url).text\n",
    "\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "    # Scrape all links on page\n",
    "\n",
    "    project_href = [i['href'] for i in soup.find_all('a', href=True)]\n",
    "    # print(project_href)\n",
    "\n",
    "    #Filter out links that are not houses for sale\n",
    "\n",
    "    for i in project_href:\n",
    "        if i.startswith('/kohde'):\n",
    "            houses_href.append(i)\n",
    "\n",
    "    time.sleep(0.25) #Lets not overload servers\n",
    "    # print(houses_href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56590ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47769\n",
      "47761\n"
     ]
    }
   ],
   "source": [
    "# Just to make sure lets remove possible duplicates.\n",
    "print(len(houses_href))\n",
    "\n",
    "houses_href = list(dict.fromkeys(houses_href))\n",
    "print(len(houses_href))\n",
    "\n",
    "# print(houses_href)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5daf30a",
   "metadata": {},
   "source": [
    "## Scraping data from the adverts\n",
    "This part iterates through all of the adverts and looks for values like type, address, price, size and year. In the end it transforms it to a pandas dataframe. NOTE: This iteration is quite large and it takes approximately 10 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27cd7c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [12:31<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "houses_for_sale = []\n",
    "\n",
    "# for href in itertools.islice(houses_href, 0, 30): # For testing\n",
    "for href in tqdm(houses_href[0:1000]):\n",
    "\n",
    "    url = \"https://www.etuovi.com/\" + str(href)\n",
    "\n",
    "    data = requests.get(url).text\n",
    "\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "    # Type, rooms, kitchen, bathroom etc.\n",
    "    try:\n",
    "        house_type_etc = [i.string for i in soup.find('div', class_='flexboxgrid__col-xs-12__1I1LS flexboxgrid__col-md-6__1n8OT ItemSummaryContainer__alignLeft__2IE5Z')][0]\n",
    "    except:\n",
    "        TypeError\n",
    "        house_type_etc = None\n",
    "        continue\n",
    "\n",
    "\n",
    "    # City\n",
    "    try:\n",
    "        house_city = [i.string for i in soup.find('a', class_='MuiTypography-root MuiLink-root MuiLink-underlineHover InfoSegment__noStyleLink__2e28Y MuiTypography-colorPrimary')][0]\n",
    "    except:\n",
    "        TypeError\n",
    "        house_city = None\n",
    "        continue\n",
    "\n",
    "\n",
    "    #Area  \n",
    "    try:\n",
    "        house_area = [i.string for i in soup.find('a', class_='MuiTypography-root MuiLink-root MuiLink-underlineHover InfoSegment__noStyleLink__2e28Y MuiTypography-colorPrimary').nextSibling][1]\n",
    "    except:\n",
    "        TypeError\n",
    "        house_area = None\n",
    "        continue \n",
    "\n",
    "\n",
    "    #Address\n",
    "    try:\n",
    "        house_address = [i.string for i in soup.find('a', class_='MuiTypography-root MuiLink-root MuiLink-underlineHover InfoSegment__noStyleLink__2e28Y MuiTypography-colorPrimary').nextSibling.parent.nextSibling][0]\n",
    "    except:\n",
    "        TypeError\n",
    "        house_address = None\n",
    "        continue \n",
    "\n",
    "\n",
    "    # Price\n",
    "    try:\n",
    "        house_price = [i.string for i in soup.find('div', class_='flexboxgrid__col-xs-4__p2Lev flexboxgrid__col-md-5__3SFMx')][1]\n",
    "    except:\n",
    "        TypeError\n",
    "        house_price = None\n",
    "        continue\n",
    "\n",
    "    # Take only the number, not unit.\n",
    "    if house_price is None:\n",
    "        house_price = None \n",
    "    else: house_price_number = house_price[0:(len(house_price)-2)]\n",
    "\n",
    "\n",
    "    # Size\n",
    "    try:\n",
    "        house_size = [i.string for i in soup.find('div', class_='flexboxgrid__col-xs-4__p2Lev flexboxgrid__col-md-4__2DYW-')][1]\n",
    "    except:\n",
    "        TypeError\n",
    "        house_size = None\n",
    "        continue\n",
    "\n",
    "    # Take only the number, not unit.\n",
    "    if house_size is None:\n",
    "        house_size_number = None\n",
    "    else: house_size_number = house_size.rsplit(' ', 100)[0]\n",
    "\n",
    "\n",
    "    # Year\n",
    "    try:\n",
    "        house_year = [i.string for i in soup.find('div', class_='flexboxgrid__col-xs-4__p2Lev flexboxgrid__col-md-3__1YPhN')][1]\n",
    "    except:\n",
    "        TypeError\n",
    "        house_year = None\n",
    "        continue\n",
    "\n",
    "\n",
    "    # \"house_type_etc\" is now in format type | number of rooms + etc. + etc.\n",
    "    # Lets divide it to own categories\n",
    "    house_type_etc = str(house_type_etc)\n",
    "    # house_type = house_type_etc.rsplit(' ', 100)[0]\n",
    "\n",
    "    houses_for_sale.append({\"Tyyppi\":house_type_etc,\"Osoite\":house_address, \"Kaupunginosa\":house_area ,\"Kaupunki\":house_city, \"Hinta (€)\":house_price_number, \"Pinta-ala (sqm.)\":house_size_number, \"Rakennusvuosi\" : house_year})\n",
    "    time.sleep(0.25)\n",
    "\n",
    "houses_df = pd.DataFrame(houses_for_sale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e5887",
   "metadata": {},
   "source": [
    "## Saving the scraped data to a .csv file for later refinement and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fe670e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jun-29-2022-house_market_data.csv\n",
      "                                                Tyyppi  \\\n",
      "0                             Kerrostalo | 2H + KK + S   \n",
      "1    Omakotitalo | 4h, k, takkah., s. 2 tallia, ver...   \n",
      "2           Kerrostalo | 3h, k, kh, s, wc, vh, parveke   \n",
      "3                Omakotitalo | 3 mh, oh, k, khh, kh, s   \n",
      "4    Kerrostalo | 3-4h + kt + s + vh + khh + las.pa...   \n",
      "..                                                 ...   \n",
      "973                                Kerrostalo | 3h + k   \n",
      "974                           Omakotitalo | 3h + k + s   \n",
      "975    Omakotitalo | 3-4h, oh, k, khh, kph / wc, s, wc   \n",
      "976  Kerrostalo | 2h + kt + lasitettu parveke ja as...   \n",
      "977                               Kerrostalo | 1h + kt   \n",
      "\n",
      "                     Osoite  Kaupunginosa     Kaupunki  Hinta (€)  \\\n",
      "0               Swingitie 8          None      Tuusula  16 712,24   \n",
      "1             Karpalotie 10      Lapijoki     Eurajoki    119 000   \n",
      "2            Kaivokatu 12 B      Keskusta  Hämeenlinna    155 000   \n",
      "3             Vesalantie 12    Oulunlahti         Oulu    340 000   \n",
      "4    Itätuulenkuja 10 C 145       Tapiola        Espoo    985 600   \n",
      "..                      ...           ...          ...        ...   \n",
      "973      Signalistinkatu 11     Runosmäki        Turku    123 000   \n",
      "974           Rösslevägen 2     Rekipelto        Vöyri     35 000   \n",
      "975      Muonamiehenkatu 16       Kytömaa       Kerava    369 000   \n",
      "976  Laajaniitynkuja 11 B 2  Martinlaakso       Vantaa    162 990   \n",
      "977           Kohmankaari 9        Tesoma      Tampere    115 000   \n",
      "\n",
      "    Pinta-ala (sqm.) Rakennusvuosi  \n",
      "0                 51          1995  \n",
      "1                120          1975  \n",
      "2                 76          1980  \n",
      "3                150          1989  \n",
      "4                112          2024  \n",
      "..               ...           ...  \n",
      "973               80          1972  \n",
      "974              100          1947  \n",
      "975              107          2019  \n",
      "976             31,5          2022  \n",
      "977             20,5          2018  \n",
      "\n",
      "[978 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "today = date.today()\n",
    "bdy = today.strftime(\"%b-%d-%Y\")\n",
    "filename = bdy + '-house_market_data.csv'\n",
    "print(filename)\n",
    "houses_df.to_csv(filename, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(houses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27b17e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Webscraping and Data Science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "735f910f74695c6de02b2f60c5488624593c6ef6110038a08f3abc746094daf6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
